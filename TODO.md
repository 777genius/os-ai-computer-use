
## TODO

- Кэширование промпта: использовать Prompt Caching для системного промпта/общих инструкций, чтобы не платить за них каждый раз (см. раздел Prompt caching в доках Anthropic).

- Кадрирование скриншотов: добавить действие screenshot_region (x,y,w,h) и просить модель снимать только локальную область (таблица/диалог), а не весь экран.

- Переключение модели: если позволяет сценарий, рассмотреть Sonnet 3.7 (часто дешевле на вход) вместо полноразмерного Claude 4 для рутинных шагов; «думать» включать точечно.





- 1) Да. Мы можем обязать модель на каждом шаге отдавать компактный «контекстный блок» (state + step) — либо как отдельный text-блок JSON, либо прямо в tool_use.input (доп. поля). В system_prompt задаём контракт, например:
  - state_update: {app, url, focus, modal, auth, last_screenshot?}
  - step_log: {action, target, intent, result, retry?, error?}
  - лимиты: ≤ 400–600 символов, без base64/скринов. Модель будет следовать этому в каждом tool_use (см. agent loop в доке Anthropic Computer Use [ссылка](https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/computer-use-tool)).

- 2) Отдельный дешёвый summarizer — хорошая альтернатива/усиление. Делать периодический вызов лёгкой модели (например, «haiku») для сжатия «старшего хвоста» сообщений в наш целевой формат (state + steps). Стоимость одной сводки обычно меньше, чем постоянная «думательная» нагрузка основной модели, а основной диалог остаётся чистым и коротким.

Рекомендую гибрид:
- Контракт на «state_update + step_log» в каждом шаге (почти нулевой оверхед).
- Периодическая свёртка истории дешёвой моделью (каждые N итераций/по превышению длины) в долговременный summary, который подмешиваем в system и обрезаем хвост.

Готов внедрить:
- Обновлю system_prompt с жёстким контрактом по полям/лимитам.
- Добавлю класс Summarizer (например, `utils/summarizers/llm_summarizer.py`) с вызовом дешёвой модели и интегрирую его в `ConversationOptimizer` (триггеры: длина/интервал).